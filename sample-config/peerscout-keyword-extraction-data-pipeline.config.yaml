gcpProjectName: 'elife-data-pipeline' #mandatory
importedTimestampFieldName: 'data_hub_imported_timestamp' #mandatory
stateFile:
  bucketName: '{ENV}-elife-data-pipeline'
  objectName: 'airflow-config/keyword-extraction/{ENV}-state/key_extraction_pipeline_state_test.json'
keywordExtractionPipelines:
  - pipelineID: keywords_from_research_interests  #mandatory
    defaultStartTimestamp: '2000-01-01 00:00:00+0000' #format must be "%Y-%m-%d %H:%M:%S%z"
    stateTimestampField: imported_timestamp
    sourceDataset: 'prod'  #mandatory
    destinationDataset: '{ENV}'  #mandatory
    destinationTable: "test_extracted_keywords"  #mandatory
    queryTemplate: |
           WITH temp_t AS(
               SELECT
                   name AS id,
                   keywords AS keywords_csv,
                   COALESCE(research_interests, "") AS text_field,
                   imported_timestamp,
                   ROW_NUMBER() OVER (PARTITION BY name
                       ORDER BY imported_timestamp DESC) AS t
               FROM `{project}.{dataset}.public_editor_profile`
           )
           SELECT * EXCEPT(t) FROM temp_t
           WHERE t=1
    # mandatory
    #to reduce properties to configure,
    #it is assumed that query template placeholders for  project, dataset, and latest state value are
    #{project}, {dataset} and {latest_state_value}
    textField: 'text_field'  #mandatory
    existingKeywordsField: 'keywords_csv'
    idField: 'id'
    importedTimestampFieldName: #
    tableWriteAppend: 'true'
    limitRowCountValue:  #config element primarily used during test
    spacyLanguageModel: 'en_core_web_lg'
    provenance:
      type: fixedValue #either sourceDataFieldName or fixedValue (DEFAULT)
      # provide EITHER the field in source data from which to
      # retrieve the value OR the fixed value to enter into provenance
      # field which indicate the kind of data source from which the keywords
      # is extracted
      value: public_editor_profile
