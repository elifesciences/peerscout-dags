version: '3.4'

x-airflow-env:
    &airflow-env
    - LOAD_EX=n
    - AIRFLOW_HOST=webserver
    - AIRFLOW_PORT=8080
    - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
    - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/1
    - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    - AIRFLOW__CORE__FERNET_KEY='81HqDtbqAywKSOumSha3BhWNOdQ26slT6K0YaZeZyPs='
    - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.default
    - DEPLOYMENT_ENV=${PEERSCOUT_DAGS_DEPLOYMENT_ENV}
    - GOOGLE_APPLICATION_CREDENTIALS=/home/airflow/.config/gcloud/credentials.json
    - EXTRACT_KEYWORDS_FILE_PATH=/home/airflow/app-config/peerscout-keyword/peerscout-keyword-data-pipeline.config.yaml

x-airflow-volumes:
    &airflow-volumes
        - ${GOOGLE_APPLICATION_CREDENTIALS:-~/.config/gcloud/application_default_credentials.json}:/tmp/credentials.json
        - ~/.aws/credentials:/tmp/.aws-credentials
        - ./test-config/peerscout-keyword-extraction-data-pipeline.config.yaml:/home/airflow/app-config/peerscout-keyword/peerscout-keyword-data-pipeline.config.yaml

services:
    peerscout-dags:
        environment:
            GOOGLE_APPLICATION_CREDENTIALS: /tmp/credentials.json
        volumes:
            - ./credentials.json:/tmp/credentials.json
        build:
            context: .
        image: elifesciences/peerscout-dags
        command: ''

    peerscout-dags-dev:
        environment:
            - DEPLOYMENT_ENV=ci
        volumes:
            - ./credentials.json:/tmp/credentials.json
        build:
            context: .
            dockerfile: Dockerfile
            args:
                install_dev: y
        image:  elifesciences/peerscout-dags-dev
        command: /bin/sh -c exit 0
        entrypoint: []

    webserver:
        depends_on:
            - worker
        environment: *airflow-env
        image:  elifesciences/peerscout-dags-dev
        entrypoint: /entrypoint
        command: webserver

    scheduler:
        image:  elifesciences/peerscout-dags-dev
        depends_on:
            - postgres
        environment: *airflow-env
        entrypoint: /entrypoint
        command: scheduler
    
    worker:
        environment: *airflow-env
        depends_on:
          - redis
          - scheduler
        volumes: *airflow-volumes
        image: elifesciences/peerscout-dags-dev
        entrypoint: /entrypoint
        hostname: worker
        command: >
            bash -c "sudo install -D /tmp/credentials.json -m 644 -t  /home/airflow/.config/gcloud
            && sudo install -D /tmp/.aws-credentials -m 644 --no-target-directory /home/airflow/.aws/credentials
            && airflow worker"

    test-client:
        image:  elifesciences/peerscout-dags-dev
        depends_on:
            - scheduler
            - webserver
        environment: *airflow-env
        volumes: *airflow-volumes
        command: >
            bash -c "sudo install -D /tmp/credentials.json -m 644 -t  /home/airflow/.config/gcloud
            && sudo install -D /tmp/.aws-credentials -m 644 --no-target-directory /home/airflow/.aws/credentials
            && ./run_test.sh with-end-to-end"

    postgres:
        image: postgres:9.6
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        healthcheck:
            test: ["CMD", "bash", "-c", "echo > /dev/tcp/localhost/5432"]
            interval: 10s
            timeout: 10s
            retries: 5
    
    redis:
        image: redis:5.0.5
        environment:
            - ALLOW_EMPTY_PASSWORD=yes

    flower:
        image: elifesciences/peerscout-dags-dev
        depends_on:
            - redis
        environment: *airflow-env
        ports:
            - "5555:5555"
        command: celery flower
